<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>If you’re building large-scale ML systems - distributed training, batch inference, or LLM serving with <strong>vLLM</strong> - combining <strong>Kubernetes + Ray + GKE</strong> gives you a powerful, production-ready stack.<d-footnote>See <a href="https://docs.ray.io/en/latest/cluster/kubernetes/index.html" rel="external nofollow noopener" target="_blank">Ray on Kubernetes</a>, <a href="https://docs.ray.io/en/latest/cluster/getting-started.html" rel="external nofollow noopener" target="_blank">Ray Clusters Overview</a>, and <a href="https://cloud.google.com/kubernetes-engine/docs/add-on/ray-on-gke" rel="external nofollow noopener" target="_blank">Ray on GKE</a> for official documentation.</d-footnote></p> <p>This post walks through:</p> <ul> <li>Architecture overview</li> <li>Setting up GKE (Standard vs Autopilot)</li> <li>Deploy Ray on Google Kubernetes Engine (GKE)</li> <li>Configure GPU-enabled Ray clusters</li> <li>Expose the Ray dashboard securely via Ingress</li> <li>Manage dependencies with <code class="language-plaintext highlighter-rouge">uv</code> </li> <li>Submit distributed jobs from your laptop or CI</li> <li>Prepare your setup for production-grade scaling</li> </ul> <hr> <h2 id="architecture-overview">Architecture Overview</h2> <p>At a high level:</p> <ul> <li> <strong>Google Kubernetes Engine (GKE)</strong> → Infrastructure &amp; orchestration</li> <li> <strong>Ray</strong> → Distributed compute engine</li> <li> <strong>PyTorch</strong> → Model training</li> <li> <strong>vLLM</strong> → High-performance LLM serving</li> </ul> <h3 id="how-it-all-fits-together">How It All Fits Together</h3> <pre><code class="language-mermaid">flowchart TD
    A[User / CI] --&gt;|Submit Job| B(GKE Ingress)
    B --&gt; C[Ray Head Pod]
    C --&gt; D[Ray Workers]
    D --&gt; E[GPU Nodes]
    C --&gt; F[Ray Dashboard 8265]
    D --&gt; G[PyTorch Training]
    D --&gt; H[vLLM Serving]
</code></pre> <hr> <h3 id="what-each-layer-does">What Each Layer Does</h3> <table> <thead> <tr> <th>Layer</th> <th>Responsibility</th> </tr> </thead> <tbody> <tr> <td>GKE</td> <td>Provisions nodes, autoscaling, networking</td> </tr> <tr> <td>KubeRay</td> <td>Manages Ray clusters as CRDs</td> </tr> <tr> <td>Ray</td> <td>Schedules distributed jobs</td> </tr> <tr> <td>vLLM</td> <td>Fast LLM inference</td> </tr> <tr> <td>PyTorch</td> <td>Training &amp; fine-tuning</td> </tr> </tbody> </table> <hr> <h2 id="prepare-your-environment">Prepare Your Environment</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">PROJECT_ID</span><span class="o">=</span>&lt;project_id&gt;
<span class="nb">export </span><span class="nv">REGION</span><span class="o">=</span>us-central1
<span class="nb">export </span><span class="nv">ZONE</span><span class="o">=</span>us-central1-a
<span class="nb">export </span><span class="nv">CLUSTER_NAME</span><span class="o">=</span>ray-cluster
<span class="nb">export </span><span class="nv">POOL_NAME</span><span class="o">=</span>gpu-node-pool
<span class="nb">export </span><span class="nv">NAMESPACE</span><span class="o">=</span>llm

gcloud config <span class="nb">set </span>project <span class="nv">$PROJECT_ID</span>
gcloud config <span class="nb">set </span>billing/quota_project <span class="nv">$PROJECT_ID</span>
gcloud services <span class="nb">enable </span>container.googleapis.com
</code></pre></div></div> <p>Connect to your cluster after creation:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud container clusters get-credentials <span class="nv">$CLUSTER_NAME</span> <span class="nt">--location</span><span class="o">=</span><span class="nv">$REGION</span>
</code></pre></div></div> <hr> <h2 id="create-a-gke-cluster">Create a GKE Cluster</h2> <aside><p><strong>GKE options:</strong> <a href="https://cloud.google.com/kubernetes-engine/docs/add-on/ray-on-gke/how-to/enable-ray-on-gke" rel="external nofollow noopener" target="_blank">Enable Ray on GKE</a> · <a href="https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.html" rel="external nofollow noopener" target="_blank">Start GKE with GPUs for KubeRay</a> (Standard or Autopilot)</p></aside> <p>You have two options. Currently I have only worked on option B.</p> <ol> <li> <p>Option A - Autopilot (Managed Mode)</p> <p>Pros:</p> <ul> <li>Less infrastructure management</li> <li>Ray operator can be enabled directly</li> </ul> <div class="language-bash highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> gcloud container clusters create-auto <span class="nv">$CLUSTER_NAME</span> <span class="se">\</span>
     <span class="nt">--location</span><span class="o">=</span><span class="nv">$REGION</span> <span class="se">\</span>
     <span class="nt">--release-channel</span><span class="o">=</span>rapid <span class="se">\</span>
     <span class="nt">--enable-ray-operator</span>
</code></pre></div> </div> <p>Autopilot currently has limitations with <code class="language-plaintext highlighter-rouge">--enable-ray-operator</code> in some regions.<d-footnote>Autopilot example: [Deploy Ray Serve Stable Diffusion on GKE](https://docs.cloud.google.com/kubernetes-engine/docs/add-on/ray-on-gke/tutorials/deploy-ray-serve-stable-diffusion#autopilot). ChatGPT guides: [Deploy RayCluster on GKE](https://chatgpt.com/share/6988d953-7834-800b-a8fd-1387e2bcedc3) · [RayCluster on GKE](https://chatgpt.com/share/6988d9ab-f750-800b-870b-f4b25bf6f281)</d-footnote></p> </li> <li> <p>Option B - Standard Cluster (More Control)</p> <p>Recommended for GPU-heavy ML workloads.<d-footnote>[AI/ML orchestration on GKE](https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra)</d-footnote></p> <div class="language-bash highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> gcloud container clusters create <span class="nv">$CLUSTER_NAME</span> <span class="se">\</span>
 <span class="nt">--zone</span><span class="o">=</span><span class="nv">$ZONE</span> <span class="se">\</span>
 <span class="nt">--machine-type</span> e2-standard-4 <span class="se">\</span>
 <span class="nt">--num-nodes</span><span class="o">=</span>1 <span class="se">\</span>
 <span class="nt">--enable-autoscaling</span> <span class="se">\</span>
 <span class="nt">--min-nodes</span><span class="o">=</span>0 <span class="nt">--max-nodes</span><span class="o">=</span>2
</code></pre></div> </div> </li> </ol> <hr> <h3 id="add-gpu-node-pool-nvidia-l4-example">Add GPU Node Pool (NVIDIA L4 Example)</h3> <aside><p><strong>GPU resources:</strong> <a href="https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/gpu.html" rel="external nofollow noopener" target="_blank">Using GPUs</a> · <a href="https://docs.cloud.google.com/kubernetes-engine/docs/how-to/serve-llm-l4-ray" rel="external nofollow noopener" target="_blank">Serve an LLM on L4 GPUs with Ray</a></p></aside> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gcloud container node-pools create <span class="nv">$POOL_NAME</span> <span class="se">\</span>
  <span class="nt">--cluster</span><span class="o">=</span><span class="nv">$CLUSTER_NAME</span> <span class="se">\</span>
  <span class="nt">--zone</span><span class="o">=</span><span class="nv">$ZONE</span> <span class="se">\</span>
  <span class="nt">--accelerator</span> <span class="nb">type</span><span class="o">=</span>nvidia-l4,count<span class="o">=</span>1 <span class="se">\</span>
  <span class="nt">--machine-type</span> g2-standard-4 <span class="se">\</span>
  <span class="nt">--enable-autoscaling</span> <span class="se">\</span>
  <span class="nt">--min-nodes</span><span class="o">=</span>0 <span class="nt">--max-nodes</span><span class="o">=</span>2
</code></pre></div></div> <p>Verify GPU and device plugin:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get nodes <span class="nt">-o</span><span class="o">=</span>custom-columns<span class="o">=</span><span class="s1">'NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu'</span>
kubectl get pods <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>nvidia-gpu-device-plugin
</code></pre></div></div> <hr> <h2 id="install-kuberay-operator-if-not-using-autopilot">Install KubeRay Operator (If Not Using Autopilot)</h2> <aside><p><strong>Reference:</strong> <a href="https://github.com/ray-project/kuberay/tree/master/helm-chart/kuberay-operator" rel="external nofollow noopener" target="_blank">KubeRay Helm charts</a> · <a href="https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/helm-chart-rbac.html" rel="external nofollow noopener" target="_blank">Helm Chart RBAC</a></p></aside> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/
helm repo update

helm <span class="nb">install </span>kuberay-operator kuberay/kuberay-operator <span class="nt">--version</span> 1.5.1
</code></pre></div></div> <p>Verify:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pods
</code></pre></div></div> <p>You should see <code class="language-plaintext highlighter-rouge">kuberay-operator</code> running.</p> <p><strong>Note:</strong> If you created the cluster with <code class="language-plaintext highlighter-rouge">--enable-ray-operator</code> (Autopilot), skip this step—the Ray operator is already installed.</p> <hr> <h3 id="kuberay-kubectl-ray-plugin-autopilot-only">KubeRay kubectl-ray Plugin (Autopilot Only)</h3> <p>For Autopilot clusters with the GKE Ray add-on, you may need the KubeRay kubectl plugin:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check your KubeRay version (from CRD annotations)</span>
kubectl get crd rayclusters.ray.io <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.metadata.annotations}'</span> <span class="p">;</span> <span class="nb">echo</span>

<span class="c"># Install kubectl-ray (replace v1.4.2 with your version)</span>
curl <span class="nt">-LO</span> https://github.com/ray-project/kuberay/releases/download/v1.4.2/kubectl-ray_v1.4.2_linux_amd64.tar.gz
<span class="nb">tar</span> <span class="nt">-xvf</span> kubectl-ray_v1.4.2_linux_amd64.tar.gz
<span class="nb">cp </span>kubectl-ray ~/.local/bin

kubectl ray version
</code></pre></div></div> <hr> <h2 id="deploy-a-gpu-enabled-raycluster">Deploy a GPU-Enabled RayCluster</h2> <aside><p><strong>Docs:</strong> <a href="https://docs.ray.io/en/latest/cluster/kubernetes/getting-started.html" rel="external nofollow noopener" target="_blank">Getting Started with KubeRay</a> · <a href="https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html" rel="external nofollow noopener" target="_blank">RayCluster Configuration</a></p></aside> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> raycluster-gpu.yaml
</code></pre></div></div> <p>Check status:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get rayclusters
kubectl get pods <span class="nt">--selector</span><span class="o">=</span>ray.io/cluster<span class="o">=</span>raycluster-gpu
</code></pre></div></div> <hr> <h3 id="raycluster-internal-structure">RayCluster Internal Structure</h3> <pre><code class="language-mermaid">flowchart TB
    subgraph GKE Cluster
        H[Ray Head Pod]
        W1[Worker Pod 1]
        W2[Worker Pod 2]
    end

    H --&gt; W1
    H --&gt; W2
    W1 --&gt; GPU1[NVIDIA GPU]
    W2 --&gt; GPU2[NVIDIA GPU]
</code></pre> <hr> <h2 id="access-ray-head-pod">Access Ray Head Pod</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">HEAD_POD</span><span class="o">=</span><span class="si">$(</span>kubectl get pods <span class="se">\</span>
  <span class="nt">--selector</span><span class="o">=</span>ray.io/node-type<span class="o">=</span><span class="nb">head</span> <span class="se">\</span>
  <span class="nt">-o</span> custom-columns<span class="o">=</span>POD:metadata.name <span class="nt">--no-headers</span><span class="si">)</span>

kubectl <span class="nb">exec</span> <span class="nt">-it</span> <span class="nv">$HEAD_POD</span> <span class="nt">--</span> bash
</code></pre></div></div> <hr> <h2 id="expose-ray-dashboard-port-8265-via-gke-ingress">Expose Ray Dashboard (Port 8265) via GKE Ingress</h2> <aside><p><strong>Reference:</strong> <a href="https://docs.ray.io/en/latest/cluster/kubernetes/k8s-ecosystem/ingress.html#gke-ingress-support" rel="external nofollow noopener" target="_blank">GKE Ingress support</a></p></aside> <p>GKE supports <code class="language-plaintext highlighter-rouge">gce</code> (external) and <code class="language-plaintext highlighter-rouge">gce-internal</code> ingress modes. For <code class="language-plaintext highlighter-rouge">gce-internal</code>, you must create a <a href="https://cloud.google.com/load-balancing/docs/proxy-only-subnets#proxy_only_subnet_create" rel="external nofollow noopener" target="_blank">Proxy-Only subnet</a>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Get Ray head service name, then update ray-dashboard-ingress.yaml</span>
kubectl get svc

kubectl apply <span class="nt">-f</span> ray-dashboard-ingress.yaml
kubectl get ingress
</code></pre></div></div> <p>After a few minutes, GKE assigns an external IP.</p> <p>Visit:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>http://&lt;EXTERNAL_IP&gt;
</code></pre></div></div> <hr> <h3 id="networking-flow">Networking Flow</h3> <pre><code class="language-mermaid">flowchart LR
    User --&gt; Ingress
    Ingress --&gt; RayHeadService
    RayHeadService --&gt; RayHeadPod
    RayHeadPod --&gt; RayWorkers
</code></pre> <hr> <h2 id="dependency-management-with-uv">Dependency Management with <code class="language-plaintext highlighter-rouge">uv</code> </h2> <p>Ray supports runtime environments.<d-footnote>Ray docs: <a href="https://docs.ray.io/en/latest/ray-core/handling-dependencies.html" rel="external nofollow noopener" target="_blank">Environment Dependencies</a></d-footnote></p> <p><strong>Important:</strong> Local Python version must match the Ray image. For example, <code class="language-plaintext highlighter-rouge">rayproject/ray:2.53.0-gpu</code> uses Python 3.10.19.</p> <hr> <h3 id="use-via-rayinit-inside-ray-pods">Use via ray.init() (Inside Ray Pods)</h3> <p>When running code directly inside the head/worker pod, use <code class="language-plaintext highlighter-rouge">runtime_env</code> with <code class="language-plaintext highlighter-rouge">uv</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv <span class="nb">export</span> <span class="nt">--format</span> requirements.txt <span class="nt">-o</span> requirements.txt
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># runtime_env expects requirements.txt, not pyproject.toml
</span><span class="n">ray</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">runtime_env</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">uv</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">./path/requirements.txt</span><span class="sh">"</span><span class="p">})</span>
</code></pre></div></div> <hr> <h3 id="pattern-a-best-for-iteration-ray-job-submit---uv-run-">Pattern A (Best for Iteration): ray job submit … – uv run …</h3> <p>Keep a repo locally (or on CI) with <code class="language-plaintext highlighter-rouge">pyproject.toml</code>, <code class="language-plaintext highlighter-rouge">uv.lock</code>, and your scripts. Submit from any machine that can reach the Ingress:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv lock

ray job submit <span class="se">\</span>
  <span class="nt">--address</span><span class="o">=</span><span class="s2">"http://&lt;INGRESS_IP_OR_DNS&gt;:8265"</span> <span class="se">\</span>
  <span class="nt">--no-wait</span> <span class="se">\</span>
  <span class="nt">--working-dir</span> <span class="nb">.</span> <span class="se">\</span>
  <span class="nt">--</span> uv run main.py
</code></pre></div></div> <p>Ray uploads your working directory and installs dependencies. Use <code class="language-plaintext highlighter-rouge">--no-wait</code> for fire-and-forget, then:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ray job logs &lt;job-id&gt; <span class="nt">--address</span><span class="o">=</span><span class="s2">"http://&lt;INGRESS_IP_OR_DNS&gt;:8265"</span>
ray job status &lt;job-id&gt; <span class="nt">--address</span><span class="o">=</span><span class="s2">"http://&lt;INGRESS_IP_OR_DNS&gt;:8265"</span>
ray job stop &lt;job-id&gt; <span class="nt">--address</span><span class="o">=</span><span class="s2">"http://&lt;INGRESS_IP_OR_DNS&gt;:8265"</span>
</code></pre></div></div> <p><strong>Tip:</strong> Set <code class="language-plaintext highlighter-rouge">export RAY_API_SERVER_ADDRESS="http://&lt;INGRESS_IP_OR_DNS&gt;:8265"</code> to avoid passing <code class="language-plaintext highlighter-rouge">--address</code> every time.</p> <hr> <h3 id="remote-working_dir-avoid-local-upload">Remote working_dir (Avoid Local Upload)</h3> <p>Instead of <code class="language-plaintext highlighter-rouge">--working-dir .</code>, use a remote URI so Ray fetches code from GitHub or GCS:</p> <table> <thead> <tr> <th>Source</th> <th>Example</th> </tr> </thead> <tbody> <tr> <td>Public GitHub</td> <td><code class="language-plaintext highlighter-rouge">https://github.com/user/repo/archive/HEAD.zip</code></td> </tr> <tr> <td>Private GitHub</td> <td><code class="language-plaintext highlighter-rouge">https://user:TOKEN@github.com/user/repo/archive/HEAD.zip</code></td> </tr> <tr> <td>GCS</td> <td><code class="language-plaintext highlighter-rouge">gs://bucket/code.zip</code></td> </tr> </tbody> </table> <p>Example with a subdirectory (e.g. <code class="language-plaintext highlighter-rouge">src/</code> in the repo):</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ray job submit <span class="se">\</span>
  <span class="nt">--address</span><span class="o">=</span><span class="s2">"http://&lt;INGRESS_IP_OR_DNS&gt;:8265"</span> <span class="se">\</span>
  <span class="nt">--working-dir</span> <span class="s2">"https://github.com/user/repo/archive/HEAD.zip"</span> <span class="se">\</span>
  <span class="nt">--</span> uv run <span class="nt">--directory</span> src main_src.py
</code></pre></div></div> <hr> <h3 id="pattern-b-best-for-production">Pattern B (Best for Production)</h3> <p>Bake dependencies into the image to avoid per-job installs:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> rayproject/ray-ml:2.x-gpu</span>
<span class="k">COPY</span><span class="s"> pyproject.toml uv.lock .</span>
<span class="k">RUN </span>uv <span class="nb">sync</span> <span class="nt">--frozen</span>
</code></pre></div></div> <p>Use this image in your RayCluster for both head and workers. Then job submission can ship only code or parameters.</p> <hr> <h3 id="option-3-remote-code-only-no-local-upload">Option 3: Remote Code Only (No Local Upload)</h3> <p>If you want to avoid uploading from your machine entirely:</p> <ol> <li>Zip your repo (single top-level directory) and upload to GCS.</li> <li>Submit with <code class="language-plaintext highlighter-rouge">--runtime-env-json</code> and <code class="language-plaintext highlighter-rouge">working_dir: "gs://bucket/code.zip"</code>:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ray job submit <span class="se">\</span>
  <span class="nt">--address</span><span class="o">=</span><span class="s2">"http://&lt;INGRESS_IP_OR_DNS&gt;:8265"</span> <span class="se">\</span>
  <span class="nt">--runtime-env-json</span><span class="o">=</span><span class="s1">'{"working_dir": "gs://bucket/code.zip"}'</span> <span class="se">\</span>
  <span class="nt">--</span> python main.py
</code></pre></div></div> <hr> <h3 id="production-workflow">Production Workflow</h3> <pre><code class="language-mermaid">flowchart TD
    Dev[Developer] --&gt;|Push Code| GitHub
    GitHub --&gt; CI
    CI --&gt;|Build Image| GCR
    GCR --&gt;|Deploy| GKE
    GKE --&gt; RayCluster
</code></pre> <hr> <h2 id="submitting-jobs-from-local--different-machines-via-ingress">Submitting Jobs from Local / Different Machines (via Ingress)</h2> <aside><p><strong>Job submission:</strong> <a href="https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayjob-quick-start.html" rel="external nofollow noopener" target="_blank">RayJob Quickstart</a> · <a href="https://docs.ray.io/en/latest/cluster/running-applications/job-submission/quickstart.html" rel="external nofollow noopener" target="_blank">Ray Jobs CLI Quickstart</a></p></aside> <p>Submit from your laptop, another engineer’s machine, or a CI runner—as long as it has the code and can reach <code class="language-plaintext highlighter-rouge">http://&lt;INGRESS_IP&gt;:8265</code>.</p> <h3 id="1-install-ray-cli">1. Install Ray CLI</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv tool <span class="nb">install</span> <span class="s2">"ray[default]"</span>   <span class="c"># runtime env feature requires ray[default]</span>
</code></pre></div></div> <h3 id="2-submit-options">2. Submit Options</h3> <table> <thead> <tr> <th>Option</th> <th>Use case</th> </tr> </thead> <tbody> <tr> <td><strong>Local dir</strong></td> <td> <code class="language-plaintext highlighter-rouge">--working-dir .</code> — uploads current directory</td> </tr> <tr> <td><strong>Remote GitHub/GCS</strong></td> <td> <code class="language-plaintext highlighter-rouge">--working-dir "https://github.com/user/repo/archive/HEAD.zip"</code> or <code class="language-plaintext highlighter-rouge">gs://bucket/code.zip</code> </td> </tr> <tr> <td><strong>Subdirectory</strong></td> <td>Add <code class="language-plaintext highlighter-rouge">-- uv run --directory src main.py</code> when code lives in a subdir</td> </tr> <tr> <td><strong>No local upload</strong></td> <td><code class="language-plaintext highlighter-rouge">--runtime-env-json='{"working_dir": "gs://bucket/code.zip"}'</code></td> </tr> </tbody> </table> <p>Example (local):</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv lock
ray job submit <span class="se">\</span>
  <span class="nt">--address</span><span class="o">=</span><span class="s2">"http://&lt;INGRESS_IP&gt;:8265"</span> <span class="se">\</span>
  <span class="nt">--working-dir</span> <span class="nb">.</span> <span class="se">\</span>
  <span class="nt">--</span> uv run main.py
</code></pre></div></div> <p>Example (remote repo + subdirectory):</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ray job submit <span class="se">\</span>
  <span class="nt">--address</span><span class="o">=</span><span class="s2">"http://&lt;INGRESS_IP&gt;:8265"</span> <span class="se">\</span>
  <span class="nt">--working-dir</span> <span class="s2">"https://github.com/user/repo/archive/HEAD.zip"</span> <span class="se">\</span>
  <span class="nt">--</span> uv run <span class="nt">--directory</span> src main.py
</code></pre></div></div> <hr> <h2 id="monitoring--autoscaling">Monitoring &amp; Autoscaling</h2> <p>You should configure:</p> <ul> <li>Ray autoscaling</li> <li>Prometheus + Grafana</li> <li>Cloud Monitoring integration</li> </ul> <p>Ray metrics default port: <strong>8080</strong></p> <hr> <h2 id="when-should-you-use-this-stack">When Should You Use This Stack?</h2> <p>Use Ray + GKE when:</p> <ul> <li>Distributed training</li> <li>Multi-GPU LLM serving</li> <li>Batch inference pipelines</li> <li>Multi-team ML platform</li> <li>CI/CD for ML infra</li> </ul> <p>Avoid if:</p> <ul> <li>Small experiments</li> <li>Single-node workloads</li> <li>No need for autoscaling</li> </ul> <hr> <h2 id="final-thoughts">Final Thoughts</h2> <p>Running <strong>Ray on GKE</strong> gives you:</p> <ul> <li>Kubernetes-native autoscaling</li> <li>GPU scheduling</li> <li>Production-ready LLM serving</li> <li>Distributed PyTorch training</li> <li>Clean job submission model</li> </ul> <p>This stack scales from experimentation → production seamlessly.</p> <aside><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=hdx0LHw7epg" rel="external nofollow noopener" target="_blank">Ray on GKE tutorial</a> on YouTube</p></aside> </body></html>